{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-load these packages on your device in order to run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/noahz/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load packages from tweepy\n",
    "import tweepy as tp\n",
    "\n",
    "#Load package from snscrape to scrape twitters frontend\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "#NLP Modules\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#Genism module\n",
    "import gensim\n",
    "\n",
    "#Load additional packages\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import re\n",
    "import sys\n",
    "from retry import retry\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweey API Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step must be repeated for each new key:value pair you'd like to store as local varaible. For the purpose of this repository, only four are required. Your twitter api key, api secret key, access token and access token secret. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an empty dictionary for you to copy paste your twitter api keys into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key = 'value'\n",
    "#twtr_api_key = ''\n",
    "#twtr_api_secret_key = ''\n",
    "#twtr_bearer_token = ''\n",
    "#twtr_access_token = ''\n",
    "#twtr_access_token_secret = ''\n",
    "#alpha_apikey = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy paste each key value pair into the code below and run it for each key, value pair in order to save these as local environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'dest' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/dotenv/main.py\u001b[0m in \u001b[0;36mrewrite\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                 \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ffd358b9dd2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'value'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Update the new dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdotenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdotenv_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/dotenv/main.py\u001b[0m in \u001b[0;36mset_key\u001b[0;34m(dotenv_path, key_to_set, value_to_set, quote_mode, export)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mline_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}={}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_to_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mrewrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdotenv_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mreplaced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwith_warn_for_invalid_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/dotenv/main.py\u001b[0m in \u001b[0;36mrewrite\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'dest' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#Look for .env file and assign file path\n",
    "dotenv_file = dotenv.find_dotenv()\n",
    "#Load the .env file for editing\n",
    "dotenv.load_dotenv(dotenv_file)\n",
    "#Overwrite the os.environ dict with new key:value pair\n",
    "os.environ['key'] = 'value'\n",
    "#Update the new dict \n",
    "dotenv.set_key(dotenv_file,'key',os.environ['key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set api and secret key variables for use in this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ['twtr_api_key']\n",
    "api_secret_key = os.environ['twtr_api_secret_key']\n",
    "bearer_token = os.environ['twtr_bearer_token']\n",
    "access_token = os.environ['twtr_access_token']\n",
    "access_token_secret = os.environ['twtr_access_token_secret']\n",
    "apikey = os.environ['alpha_apikey']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorize api keys in order to use api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authorization using api keys\n",
    "auth = tp.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tp.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stock name and symbol data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import historical stock symbol and name data from csv assets in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import S&P 500 stocks\n",
    "s_p = pd.read_csv('data/constituents_csv.csv')\n",
    "s_p.drop(['Sector'],axis=1,inplace=True)\n",
    "s_p['source'] = 's_p'\n",
    "\n",
    "#Import nasdaq stocks\n",
    "nasdaq = pd.read_csv('data/nasdaq-listed_csv.csv')\n",
    "nasdaq.drop(['Security Name','Market Category','Test Issue','Financial Status','Round Lot Size'],axis=1,inplace=True)\n",
    "nasdaq.rename(columns={'Company Name':'Name'},inplace=True)\n",
    "nasdaq['source'] = 'nasdaq'\n",
    "\n",
    "#Import nyse stocks\n",
    "nyse = pd.read_csv('data/nyse-listed_csv.csv')\n",
    "nyse.rename(columns={'ACT Symbol':'Symbol','Company Name':'Name'},inplace=True)\n",
    "nyse['source'] = 'nyse'\n",
    "\n",
    "#Import tsx stocks \n",
    "tsx = pd.read_csv('data/TSX.txt',sep='\\t')\n",
    "tsx.rename(columns={'Description':'Name'},inplace=True)\n",
    "tsx['source'] = 'tsx'\n",
    "\n",
    "#Import tsxv stocks \n",
    "tsxv = pd.read_csv('data/TSXV.txt',sep='\\t')\n",
    "tsxv.rename(columns={'Description':'Name'},inplace=True)\n",
    "tsxv['source'] = 'tsxv'\n",
    "\n",
    "#Import nyse list 2 stocks \n",
    "nyse_2 = pd.read_csv('data/NYSE.txt',sep='\\t')\n",
    "nyse_2.rename(columns={'Description':'Name'},inplace=True)\n",
    "nyse_2['source'] = 'nyse'\n",
    "\n",
    "#Append to single list and remove any duplicates\n",
    "stock_names = s_p.append(nasdaq).append(nyse).append(tsx).append(tsxv).append(nyse_2)\n",
    "stock_names['id'] = stock_names['Symbol'] + stock_names['source']\n",
    "stock_names.drop_duplicates(subset='id',inplace=True)\n",
    "stock_names.dropna(inplace=True)\n",
    "stock_names.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull historical stock information for GME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab GME price data from Jan 1st, 2021 to March 5th, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.alphavantage.co/query\"\n",
    "req_av = requests.get(base_url,params={'function':'TIME_SERIES_DAILY',\n",
    "                                       'symbol':'GME',\n",
    "                                      'apikey':apikey,'outputsize':'full'}) \n",
    "\n",
    "req_av.status_code\n",
    "req_av.url\n",
    "gme = req_av.json()\n",
    "gme['Time Series (Daily)']\n",
    "df = pd.DataFrame(gme['Time Series (Daily)'])\n",
    "df = df.T\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df = df.sort_index()\n",
    "df = df['2021']\n",
    "df['1. open'] = pd.to_numeric(df['1. open'], downcast=\"float\")\n",
    "df['2. high'] = pd.to_numeric(df['2. high'], downcast=\"float\")\n",
    "df['3. low'] = pd.to_numeric(df['3. low'], downcast=\"float\")\n",
    "df['4. close'] = pd.to_numeric(df['4. close'], downcast=\"float\")\n",
    "df['5. volume'] = pd.to_numeric(df['5. volume'], downcast=\"float\")\n",
    "s= df['4. close']\n",
    "idx = pd.date_range('01-01-2021', '03-05-2021')\n",
    "s = s.reindex(idx,method='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to loop through a list of twitter account handles and pull their last num_posts using the tweepy API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initilization\n",
    "array = [[]]\n",
    "\n",
    "#Pass in any series list of twitter account names\n",
    "@retry(tries=10,delay=2,backoff=4,max_delay=42)\n",
    "def get_top_tweets(s,num_posts):\n",
    "    for screen_name in s:\n",
    "        try:\n",
    "            #Only pull the last __ number of tweets from each account \n",
    "            for tweet in api.user_timeline(screen_name = screen_name, count = num_posts):\n",
    "                \n",
    "                #Initialize temp lists as empty\n",
    "                data = []\n",
    "                hashtag_list = []\n",
    "                \n",
    "                #Append tweet metadata to temp lists\n",
    "                data.append(f'{screen_name}')\n",
    "                data.append(tweet.created_at) \n",
    "                data.append(tweet.text)\n",
    "                data.append(tweet.retweet_count)\n",
    "                data.append(tweet.favorite_count)\n",
    "                if len(tweet.entities.get('hashtags'))>0:\n",
    "                    ht = [tweet.entities.get('hashtags')[x]['text'] for x in range(0,len(tweet.entities.get('hashtags')))]\n",
    "                    hashtag_list.append(ht)\n",
    "                data.append(hashtag_list)\n",
    "                \n",
    "                #Append lists to array\n",
    "                array.append(data)\n",
    "                \n",
    "                #Reset temp lists to empty\n",
    "                data = []\n",
    "                hashtag_list = []\n",
    "        \n",
    "        #Pass over an account name if it is no longer active\n",
    "        except tp.TweepError:\n",
    "            time.sleep(60*15)\n",
    "            continue\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to convert a list of items into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to convert list items to a single string joined by semicolon \n",
    "def list_to_string(x):\n",
    "    lis = x\n",
    "    string = ''\n",
    "    string = \" \".join(lis)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract the hastags from the collected dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hts_extract(df):\n",
    "    #Convert hashtags columns to a list\n",
    "    hashtags = df['hashtags'].tolist()\n",
    "\n",
    "    #Initialize two empty lists for temp storage\n",
    "    hts = []\n",
    "    ht = []\n",
    "    ht_final = []\n",
    "\n",
    "    #Iterate through and remove blank hashtags\n",
    "    for x in hashtags:\n",
    "        if x == '':\n",
    "            pass\n",
    "        else:\n",
    "            hts.append(x)\n",
    "\n",
    "    #Iterate through and split out multiple hastags into additional list elements\n",
    "    for x in hts:\n",
    "        if ';' in x:\n",
    "            el = x.split(';')\n",
    "            ht = ht + el\n",
    "        else:\n",
    "            ht.append(x)\n",
    "\n",
    "    for item in ht:\n",
    "        ht_final.append(item.lower())\n",
    "    \n",
    "    #Count the number of times a hastag was mentioned, store this in a dictionary along with the keyword \n",
    "    #Sort the dictionary from highest to lowest values\n",
    "    top_hts = dict(sorted(Counter(ht_final).items(), key=lambda item: item[1],reverse=True))\n",
    "    return top_hts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to tokenize, lemmatize and remove stopwords from the tweet corpus. Custom stopwords can be added in order to remove additional noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(df):\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\.]\\S+')\n",
    "    lem = WordNetLemmatizer()\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    custom_stopwords = ['co','http','The','RT','I','day','We','This','ha','like','A','amp','If','year',\n",
    "                      'morning','since','1','2','3','4','5','6','7','8','9','That','It','right','know','Here']\n",
    "    custom_stop = set(custom_stopwords)\n",
    "    df['processed_tweets'] = df['tweet'].apply(tokenizer.tokenize)\n",
    "    df['processed_tweets'] = df['processed_tweets'].apply(lambda row: list([lem.lemmatize(i) for i in row]))\n",
    "    df['processed_tweets'] = df['processed_tweets'].apply(lambda x:[i for i in x if i not in STOPWORDS] )\n",
    "    df['processed_tweets'] = df['processed_tweets'].apply(lambda x:[i for i in x if i not in custom_stop] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to scrape twitters front end using a list of keywords,start and end date. This function uses the snscrape module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(tries=10,delay=2,backoff=4,max_delay=42)\n",
    "def get_relevant_tweets(search_words,num_queries, start_date, end_date):\n",
    "    \n",
    "    #Intitilize empty list \n",
    "    tweets_list = [] \n",
    "\n",
    "    #For loop to go through each of the relevant keyword \n",
    "    for word in search_words:\n",
    "\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{word} since:{start_date} until:{end_date}').get_items()):\n",
    "            if i>num_queries:\n",
    "                break\n",
    "            tweets_list.append([tweet.id,tweet.username,tweet.date,tweet.content,word])\n",
    "    \n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to scrate twitters front end using account name. This function can go back in time as far as needed as it is nto limited by tweepy's 7 day historical api limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(tries=10,delay=2,backoff=4,max_delay=42)\n",
    "def get_historical_tweets(s,num_queries):\n",
    "    \n",
    "    #Intitilize empty list \n",
    "    tweets_list = [] \n",
    "\n",
    "    #For loop to go through each of the relevant keyword \n",
    "    for screen_name in s:\n",
    "\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'from:{screen_name}').get_items()):\n",
    "            if i>num_queries:\n",
    "                break\n",
    "            tweets_list.append([tweet.id,tweet.username,tweet.date,tweet.content])\n",
    "    \n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list of the top twitter accounts you would like to scrape. The list has been pre-populated with popular investment accounts from primentn investment professionals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of top twitter investmenet accounts\n",
    "top_accounts = ['elonmusk','chamath','fundstrat','elerianm','CNBC','PeterLBrandt','SJosephBurns','IBDinvestors',\n",
    "               'TheStalwart','jimcramer','bespokeinvest','steve_hanke','MarketWatch','wallstreetbets','WSBChairman']\n",
    "s = pd.Series(top_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull the follower count for each account and create a dictionary with this key value pair\n",
    "follower_count = []\n",
    "top_account_dic = {}\n",
    "for account in s:\n",
    "    follower_count.append(api.get_user(account).followers_count)\n",
    "\n",
    "top_account_dic = {k:v for k,v in zip(s,follower_count)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the word2vec model from the assets folder. Ensure to replace file path with your own path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396.2274160385132\n"
     ]
    }
   ],
   "source": [
    "# Start timer.\n",
    "t0 = time.time()\n",
    "\n",
    "# Import word vectors into \"model.\"\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/noahz/Desktop/Desktop/GA/project-6-capstone/assets/lexvec.commoncrawl.300d.W.pos.vectors')\n",
    "\n",
    "# Print results of timer.\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNS scrape pull used to get historical data for model training. 500 posts for every account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.44915318489075\n"
     ]
    }
   ],
   "source": [
    "#Reset tweet list to empty\n",
    "tweets_list = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "#Set the number of posts per account to be 500\n",
    "tweets_list = get_historical_tweets(s,500)\n",
    "print(time.time() - t0)\n",
    "\n",
    "#Put the tweets list pull into a dataframe\n",
    "df_sns_1 = pd.DataFrame(tweets_list, columns=['id','screen_name','created_at','tweet'])\n",
    "df_sns_1.dropna(inplace = True)\n",
    "df_sns_1.drop_duplicates(inplace=True)\n",
    "df_sns_1.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#Profcess the tweet text\n",
    "process_tweet(df_sns_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pullout list of most frequent words \n",
    "words=[]\n",
    "for x in df_sns_1['processed_tweets']:\n",
    "    words += x\n",
    "\n",
    "words = [i.lower() for i in words]\n",
    "    \n",
    "words_dic_sorted = dict(sorted(Counter(words).items(), key=lambda item: item[1],reverse=True))\n",
    "words_dic_sorted_filt = {k: v for k, v in words_dic_sorted.items() if v > 100}\n",
    "\n",
    "#Assign most frequently occuring keywords (greater than 100 occurences) to a new list named key words \n",
    "key_words = []\n",
    "for key,value in words_dic_sorted_filt.items():\n",
    "    key_words.append(key)\n",
    "\n",
    "#Using the word2vec model, create a list of the most similar 3 words to every key words on the keywords list\n",
    "similar_words = []\n",
    "\n",
    "for i in key_words: \n",
    "    try:\n",
    "        x = model.most_similar(i, topn=3)\n",
    "        for word in range(len(x)):\n",
    "            similar_words.append(x[word][0])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "#Remove duplicated by assigning the similar words list to a set\n",
    "similar_words_set = set(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all of the @s in order to build a list of accounts to conduct a 2nd layer network twitter scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract mentions to use in 2nd level network search\n",
    "matches = []\n",
    "for word in df_sns_1['tweet']:\n",
    "    i = re.findall(r\"\\s([@][\\w_-]+)\",word,re.MULTILINE)\n",
    "    if i:\n",
    "        matches.append(i)\n",
    "\n",
    "merged = list(itertools.chain(*matches))\n",
    "s_ats = set(merged)\n",
    "s_ats_list = list(s_ats)\n",
    "s_ats_list = [x.replace('@','') for x in s_ats_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the new list of accounts to conduct an additional front end scrape using snscrape to pull the last 500 posts for every account in the 2nd layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/2/search/adaptive.json?include_profile_interstitial_type=1&include_blocking=1&include_blocked_by=1&include_followed_by=1&include_want_retweets=1&include_mute_edge=1&include_can_dm=1&include_can_media_tag=1&skip_status=1&cards_platform=Web-12&include_cards=1&include_composer_source=true&include_ext_alt_text=true&include_reply_count=1&tweet_mode=extended&include_entities=true&include_user_entities=true&include_ext_media_color=true&include_ext_media_availability=true&send_error_codes=true&simple_quoted_tweets=true&q=from%3APrestonPysh&tweet_search_mode=live&count=100&query_source=spelling_expansion_revert_click&cursor=scroll%3AthGAVUV0VFVBaAgLP14IT47SUWgMCwsZa_7_olEnEVnPl5FYCJehgHREVGQVVMVDUBFQAVAAA%3D&pc=1&spelling_corrections=1&ext=mediaStats%252CcameraMoment: ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='api.twitter.com', port=443): Read timed out. (read timeout=10)\")), retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4846.928550004959\n"
     ]
    }
   ],
   "source": [
    "#Reset tweet list to empty\n",
    "tweets_list = []\n",
    "\n",
    "t0 = time.time()\n",
    "tweets_list = get_historical_tweets(s_ats_list,500)\n",
    "print(time.time() - t0)\n",
    "\n",
    "#Put the tweets list pull into a dataframe\n",
    "df_sns_2 = pd.DataFrame(tweets_list, columns=['id','screen_name','created_at','tweet'])\n",
    "df_sns_2.dropna(inplace = True)\n",
    "df_sns_2.drop_duplicates(inplace=True)\n",
    "df_sns_2.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second layer content scrape using word2vec similar words and keywords\n",
    "Combine the key words and word2vec similar words to scrape twitters front end betwen Jan 1, 2021 and March 5, 2021 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine key words, similar words from word2vec and top hashtags + top_hts_list when doing live pull\n",
    "search_words = list(set(key_words + list(similar_words_set)))\n",
    "\n",
    "#Reset tweet list to empty\n",
    "tweets_list = []\n",
    "\n",
    "#Call the get relevant tweets function to query all twitter posts for content matching our word2vec keywords\n",
    "tweets_list = get_relevant_tweets(search_words = search_words, num_queries=50, \n",
    "                                  start_date = '2021-1-01', end_date = '2021-3-05')\n",
    "\n",
    "#Put the tweets list pull into a dataframe\n",
    "df_sns_3 = pd.DataFrame(tweets_list, columns=['id','screen_name','created_at','tweet','key_word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all of the scraped dataframes into a single master dataframe named tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process each dataframe to be combined\n",
    "df_sns_1.drop(['processed_tweets'],axis=1,inplace=True)\n",
    "df_sns_3.drop(['key_word'],axis=1,inplace=True)\n",
    "\n",
    "#Combine all dataframes into a single one\n",
    "tweets = df_sns_1.append(df_sns_2).append(df_sns_3)\n",
    "tweets.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate a column in the master dataframe with the account follower values from the created dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-76497c1e72b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'followers'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'screen_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfollower_count_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "follower_count_dic = {}\n",
    "\n",
    "for screen_name in tweets['screen_name'].unique():\n",
    "    try:\n",
    "        user = api.get_user(screen_name) \n",
    "        followers_count = user.followers_count\n",
    "        time.sleep(0.001)\n",
    "        if followers_count > 10000:\n",
    "            follower_count_dic[screen_name] = followers_count\n",
    "        else:\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "tweets['followers'] = tweets['screen_name'].map(follower_count_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and isolate any mention of a stock symbol within a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breakout the full tweets into a list of lematized and tokenized words\n",
    "process_tweet(tweets)\n",
    "\n",
    "#Extract mentions of stock symbols into a standardized new column using regex\n",
    "tweets['stocks'] = tweets['tweet'].str.extract('([$][A-Za-z][\\S]*)')\n",
    "tweets['stocks'] = tweets['stocks'].str.replace(r'[^A-Za-z0-9]+', '')\n",
    "tweets['stocks'] = tweets['stocks'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export processed data to a csv in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('data/twitter_historical_05.03.21.csv')\n",
    "stock_names.to_csv('data/stock_names.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
