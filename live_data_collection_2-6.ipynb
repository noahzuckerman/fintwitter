{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-load these packages on your device in order to run this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load packages from tweepy\n",
    "import tweepy as tp\n",
    "\n",
    "#Load package from snscrape to scrape twitters frontend\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "#NLP Modules\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#Genism module\n",
    "import gensim\n",
    "\n",
    "#Load additional packages\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import re\n",
    "import sys\n",
    "from retry import retry\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweey API Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step must be repeated for each new key:value pair you'd like to store as local varaible. For the purpose of this repository, only four are required. Your twitter api key, api secret key, access token and access token secret. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an empty dictionary for you to copy paste your twitter api keys into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#key = 'value'\n",
    "#twtr_api_key = ''\n",
    "#twtr_api_secret_key = ''\n",
    "#twtr_bearer_token = ''\n",
    "#twtr_access_token = ''\n",
    "#twtr_access_token_secret = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy paste each key value pair into the code below and run it for each key, value pair in order to save these as local environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 'key', 'value')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look for .env file and assign file path\n",
    "dotenv_file = dotenv.find_dotenv()\n",
    "#Load the .env file for editing\n",
    "dotenv.load_dotenv(dotenv_file)\n",
    "#Overwrite the os.environ dict with new key:value pair\n",
    "os.environ['key'] = 'value'\n",
    "#Update the new dict \n",
    "dotenv.set_key(dotenv_file,'key',os.environ['key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set api and secret key variables for use in this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ['twtr_api_key']\n",
    "api_secret_key = os.environ['twtr_api_secret_key']\n",
    "bearer_token = os.environ['twtr_bearer_token']\n",
    "access_token = os.environ['twtr_access_token']\n",
    "access_token_secret = os.environ['twtr_access_token_secret']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorize api keys in order to use api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Authorization using api keys\n",
    "auth = tp.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tp.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to loop through a list of twitter account handles and pull their last num_posts using the tweepy API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initilization\n",
    "array = [[]]\n",
    "\n",
    "#Pass in any series list of twitter account names\n",
    "@retry(tries=10,delay=2,backoff=4,max_delay=42)\n",
    "def get_top_tweets(s,num_posts):\n",
    "    for screen_name in s:\n",
    "        try:\n",
    "            #Only pull the last __ number of tweets from each account \n",
    "            for tweet in api.user_timeline(screen_name = screen_name, count = num_posts):\n",
    "                \n",
    "                #Initialize temp lists as empty\n",
    "                data = []\n",
    "                \n",
    "                #Append tweet metadata to temp lists\n",
    "                data.append(f'{screen_name}')\n",
    "                data.append(tweet.created_at) \n",
    "                data.append(tweet.text)\n",
    "                data.append(tweet.retweet_count)\n",
    "                data.append(tweet.favorite_count)\n",
    "                \n",
    "                #Append lists to array\n",
    "                array.append(data)\n",
    "                \n",
    "                #Reset temp lists to empty\n",
    "                data = []\n",
    "        \n",
    "        #Pass over an account name if it is no longer active\n",
    "        except tp.TweepError:\n",
    "            time.sleep(60*15)\n",
    "            continue\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to convert a list of items into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to convert list items to a single string joined by semicolon \n",
    "def list_to_string(x):\n",
    "    lis = x\n",
    "    string = ''\n",
    "    string = \" \".join(lis)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to tokenize, lemmatize and remove stopwords from the tweet corpus. Custom stopwords can be added in order to remove additional noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(df):\n",
    "    tokenizer = RegexpTokenizer('\\w+|\\$[\\.]\\S+')\n",
    "    lem = WordNetLemmatizer()\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    custom_stopwords = ['co','http','The','RT','I','day','We','This','ha','like','A','amp','If','year',\n",
    "                      'morning','since','1','2','3','4','5','6','7','8','9','That','It','right','know','Here']\n",
    "    custom_stop = set(custom_stopwords)\n",
    "    df['processed_tweets'] = df['tweet'].apply(tokenizer.tokenize)\n",
    "    df['processed_tweets'] = df['processed_tweets'].apply(lambda row: list([lem.lemmatize(i) for i in row]))\n",
    "    df['processed_tweets'] = df['processed_tweets'].apply(lambda x:[i for i in x if i not in STOPWORDS] )\n",
    "    df['processed_tweets'] = df['processed_tweets'].apply(lambda x:[i for i in x if i not in custom_stop] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to scrape twitters front end using a list of keywords,start and end date. This function uses the snscrape module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(tries=10,delay=2,backoff=4,max_delay=42)\n",
    "def get_relevant_tweets(search_words,num_queries, start_date, end_date):\n",
    "    \n",
    "    #Intitilize empty list \n",
    "    tweets_list = [] \n",
    "\n",
    "    #For loop to go through each of the relevant keyword \n",
    "    for word in search_words:\n",
    "\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'{word} since:{start_date} until:{end_date}').get_items()):\n",
    "            if i>num_queries:\n",
    "                break\n",
    "            tweets_list.append([tweet.id,tweet.username,tweet.date,tweet.content,word])\n",
    "    \n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to scrate twitters front end using account name. This function can go back in time as far as needed as it is nto limited by tweepy's 7 day historical api limit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(tries=10,delay=2,backoff=4,max_delay=42)\n",
    "def get_historical_tweets(s,num_queries):\n",
    "    \n",
    "    #Intitilize empty list \n",
    "    tweets_list = [] \n",
    "\n",
    "    #For loop to go through each of the relevant keyword \n",
    "    for screen_name in s:\n",
    "\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(f'from:{screen_name}').get_items()):\n",
    "            if i>num_queries:\n",
    "                break\n",
    "            tweets_list.append([tweet.id,tweet.username,tweet.date,tweet.content])\n",
    "    \n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list of the top twitter accounts you would like to scrape. The list has been pre-populated with popular investment accounts from primentn investment professionals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of top twitter investmenet accounts\n",
    "top_accounts = ['elonmusk','chamath','fundstrat','elerianm','CNBC','PeterLBrandt','SJosephBurns','IBDinvestors',\n",
    "               'TheStalwart','jimcramer','bespokeinvest','steve_hanke','MarketWatch','wallstreetbets','WSBChairman']\n",
    "s = pd.Series(top_accounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull the follower count for each account and create a dictionary with this key value pair\n",
    "follower_count = []\n",
    "top_account_dic = {}\n",
    "for account in s:\n",
    "    follower_count.append(api.get_user(account).followers_count)\n",
    "\n",
    "top_account_dic = {k:v for k,v in zip(s,follower_count)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the word2vec model from the assets folder. Ensure to replace file path with your own path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423.77575492858887\n"
     ]
    }
   ],
   "source": [
    "# Start timer.\n",
    "t0 = time.time()\n",
    "\n",
    "# Import word vectors into \"model.\"\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/noahz/Desktop/Desktop/GA/project-6-capstone/assets/lexvec.commoncrawl.300d.W.pos.vectors')\n",
    "\n",
    "# Print results of timer.\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweepy API Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.617655038833618\n"
     ]
    }
   ],
   "source": [
    "#Run the get top tweets function for historical tweets \n",
    "t0 = time.time()\n",
    "array = get_top_tweets(s,10)\n",
    "print(time.time() - t0)\n",
    "\n",
    "df_1 = pd.DataFrame(array,columns=['screen_name','created_at','tweet','retweets','likes'])\n",
    "df_1.dropna(inplace = True)\n",
    "\n",
    "#Remove any dduplicate values in case accounts have not posted anything new\n",
    "df_1.drop_duplicates(inplace=True)\n",
    "df_1.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#Pre-process the tweets\n",
    "process_tweet(df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahz/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py:2389: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (m / dist).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "#Pullout list of most frequent words \n",
    "words=[]\n",
    "for x in df_1['processed_tweets']:\n",
    "    words += x\n",
    "\n",
    "words = [i.lower() for i in words]\n",
    "    \n",
    "words_dic_sorted = dict(sorted(Counter(words).items(), key=lambda item: item[1],reverse=True))\n",
    "words_dic_sorted_filt = {k: v for k, v in words_dic_sorted.items() if v > 5}\n",
    "\n",
    "#Assign most frequently occuring keywords (greater than 100 occurences) to a new list named key words \n",
    "key_words = []\n",
    "for key,value in words_dic_sorted_filt.items():\n",
    "    key_words.append(key)\n",
    "\n",
    "#Using the word2vec model, create a list of the most similar 3 words to every key words on the keywords list\n",
    "similar_words = []\n",
    "\n",
    "for i in key_words: \n",
    "    try:\n",
    "        x = model.most_similar(i, topn=3)\n",
    "        for word in range(len(x)):\n",
    "            similar_words.append(x[word][0])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "#Remove duplicated by assigning the similar words list to a set\n",
    "similar_words_set = set(similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all of the @s in order to build a list of accounts to conduct a 2nd layer network twitter scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract mentions to use in 2nd level network search\n",
    "matches = []\n",
    "for word in df_1['tweet']:\n",
    "    i = re.findall(r\"\\s([@][\\w_-]+)\",word,re.MULTILINE)\n",
    "    if i:\n",
    "        matches.append(i)\n",
    "\n",
    "merged = list(itertools.chain(*matches))\n",
    "s_ats = set(merged)\n",
    "s_ats_list = list(s_ats)\n",
    "s_ats_list = [x.replace('@','') for x in s_ats_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the new list of accounts to conduct an additional front end scrape using snscrape to pull the last 500 posts for every account in the 2nd layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.15925407409668\n"
     ]
    }
   ],
   "source": [
    "#Reset tweet list to empty\n",
    "tweets_list = []\n",
    "\n",
    "t0 = time.time()\n",
    "tweets_list = get_top_tweets(s_ats_list,10)\n",
    "print(time.time() - t0)\n",
    "\n",
    "#Put the tweets list pull into a dataframe\n",
    "df_2 = pd.DataFrame(tweets_list, columns=['screen_name','created_at','tweet','retweets','likes'])\n",
    "\n",
    "df_2.dropna(inplace = True)\n",
    "\n",
    "#Remove any dduplicate values in case accounts have not posted anything new\n",
    "df_2.drop_duplicates(inplace=True)\n",
    "df_2.reset_index(drop=True,inplace=True)\n",
    "\n",
    "#Pre-process the tweets\n",
    "process_tweet(df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second layer content scrape using word2vec similar words and keywords\n",
    "Combine the key words and word2vec similar words to scrape twitters front end yesterday and today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull in today and yesterdays date \n",
    "end_date = datetime.date.today()\n",
    "start_date = end_date - datetime.timedelta(days = 1) \n",
    "end_date_str = end_date.strftime(\"%Y-%-m-%d\")\n",
    "start_date_str = start_date.strftime(\"%Y-%-m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine key words, similar words from word2vec \n",
    "search_words = list(set(key_words + list(similar_words_set)))\n",
    "\n",
    "#Reset tweet list to empty\n",
    "tweets_list = []\n",
    "\n",
    "#Call the get relevant tweets function to query all twitter posts for content matching our word2vec keywords\n",
    "tweets_list = get_relevant_tweets(search_words = search_words, num_queries=50, \n",
    "                                  start_date = start_date_str, end_date = end_date_str)\n",
    "\n",
    "#Put the tweets list pull into a dataframe\n",
    "df_3 = pd.DataFrame(tweets_list, columns=['id','screen_name','created_at','tweet','key_word'])\n",
    "\n",
    "#Pre-process the tweets\n",
    "process_tweet(df_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all of the scraped dataframes into a single master dataframe named tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['key_word'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-b2117421847a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'retweets'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'likes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'key_word'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4161\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4162\u001b[0m         \"\"\"\n\u001b[0;32m-> 4163\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   4164\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4165\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3885\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3886\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3887\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3889\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3919\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3920\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3921\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3922\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5281\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5282\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5283\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['key_word'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#Process each dataframe to be combined\n",
    "df_3['retweets'] = 0\n",
    "df_3['likes'] = 0\n",
    "df_3.drop(['key_word'],axis=1,inplace=True)\n",
    "df_3.drop(['id'],axis=1,inplace=True)\n",
    "df_3.drop(['processed_tweets'],axis=1,inplace=True)\n",
    "df_2.drop(['processed_tweets'],axis=1,inplace=True)\n",
    "df_1.drop(['processed_tweets'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "#Combine all dataframes into a single one\n",
    "tweets = df_1.append(df_2).append(df_3)\n",
    "tweets.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate a column in the master dataframe with the account follower values from the created dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "follower_count_dic = {}\n",
    "\n",
    "for screen_name in tweets['screen_name'].unique():\n",
    "    try:\n",
    "        user = api.get_user(screen_name) \n",
    "        followers_count = user.followers_count\n",
    "        time.sleep(0.001)\n",
    "        if followers_count > 10000:\n",
    "            follower_count_dic[screen_name] = followers_count\n",
    "        else:\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "tweets['followers'] = tweets['screen_name'].map(follower_count_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and isolate any mention of a stock symbol within a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breakout the full tweets into a list of lematized and tokenized words\n",
    "process_tweet(tweets)\n",
    "\n",
    "#Extract mentions of stock symbols into a standardized new column using regex\n",
    "tweets['stocks'] = tweets['tweet'].str.extract('([$][A-Za-z][\\S]*)')\n",
    "tweets['stocks'] = tweets['stocks'].str.replace(r'[^A-Za-z0-9]+', '')\n",
    "tweets['stocks'] = tweets['stocks'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export processed data to a csv in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(f'data/twitter_live_{end_date}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
